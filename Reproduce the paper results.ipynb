{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from skimage import io\n",
    "from skimage.transform import rescale\n",
    "import datetime\n",
    "import math "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Custom_Loss(nn.modules.Module):\n",
    "    def __init__(self):\n",
    "        super(Custom_Loss, self).__init__()\n",
    "        self.N = trainloader.onehot.shape[0]\n",
    "        self.Nm = np.array(trainloader.countlist)\n",
    "        self.W0 = [max( x /self.N,0.1) for x in self.Nm]\n",
    "        self.W = [(1-w0)/w0 for w0 in self.W0]\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        N = input.shape[0]\n",
    "        M = target.shape[1]\n",
    "        T = torch.tensor(self.W)\n",
    "        T = T.type(input.type())\n",
    "        result = torch.tensor(0)\n",
    "        result = result.type(output.type())\n",
    "        \n",
    "        for n in range(N):\n",
    "            #result += sum((T*target[n]*torch.log10(abs(input[n])+1e-16)+(1-target[n])*torch.log10(1-abs(input[n]))))\n",
    "            result += -1*sum(T*target[n]*(torch.log_softmax(input[n],dim=0))+(1-target[n])*(torch.log_softmax(1-input[n],dim=0)))\n",
    "                \n",
    "        result = (1/(N*M))*result\n",
    "       \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainloader:\n",
    "    \n",
    "    def __init__(self,classfolder,croppedfolder,picturesize,batchsize,split_ratio):\n",
    "        self.classdirectory = classfolder\n",
    "        self.cropdirectory = croppedfolder\n",
    "        self.picturesize = picturesize\n",
    "        self.split_ration = split_ratio\n",
    "        self.batchsize = batchsize\n",
    "        self.classlist = self._getclasses()\n",
    "        self.countlist = self._get_countlist()\n",
    "        self.onehot = self._get_onehot()\n",
    "        self.trainsplit,self.validsplit = self._train_valid_split(split_ratio)\n",
    "        self.batchnumber = 0\n",
    "        \n",
    "    \n",
    "    def _getclasses(self):\n",
    "        classlist = [x[1] for x in os.walk(self.classdirectory)][0]\n",
    "        classlist.sort()\n",
    "        \n",
    "        return classlist\n",
    "    \n",
    "    def _get_countlist(self):\n",
    "        countlist = []\n",
    "\n",
    "        for i, klasse in enumerate(self.classlist):\n",
    "            output = os.listdir(\"{}{}\".format(self.classdirectory,klasse))    \n",
    "            path, dirs, files = next(os.walk(\"{}{}\".format(self.classdirectory,klasse)))\n",
    "            file_count = len(files)\n",
    "            countlist.append(file_count)\n",
    "\n",
    "        return countlist\n",
    "    \n",
    "    def _get_onehot(self):\n",
    "        for i,currentClass in enumerate(self.classlist):\n",
    "            pictures = os.listdir(\"{}/{}\".format(self.classdirectory,currentClass))\n",
    "            df1 = pd.DataFrame([pictures,[1 for i in range(len(pictures))]]).T\n",
    "            df1.columns=['picture',currentClass]\n",
    "            if i != 0:\n",
    "                onehot = pd.merge(df1,onehot,on='picture',how='outer')\n",
    "            else:\n",
    "                onehot = df1.copy()\n",
    "        \n",
    "        onehot = onehot.fillna(value=0)\n",
    "        onehot = onehot.set_index('picture')\n",
    "        return onehot\n",
    "    \n",
    "    def _convert_picture(self,path):\n",
    "        image = io.imread(path)\n",
    "        rescale_factor = self.picturesize / image.shape[0],self.picturesize / image.shape[1]\n",
    "        scaled = rescale(image, rescale_factor, anti_aliasing=False, multichannel=True,mode='constant')[:,:,:3]\n",
    "        scaled = scaled.transpose((-1, 0, 1))\n",
    "\n",
    "        return scaled\n",
    "    \n",
    "    def _get_pictures(self,paths):\n",
    "        trainlist = []\n",
    "        for path in paths:\n",
    "            trainlist.append(self._convert_picture(self.cropdirectory+path))\n",
    "            \n",
    "        return trainlist\n",
    "    \n",
    "    def _train_valid_split(self,split_ratio):\n",
    "        split = self.onehot.index.tolist()\n",
    "        \n",
    "        \n",
    "        train = split[:int(len(split)*split_ratio)]\n",
    "        valid = split[int(len(split)*split_ratio):]\n",
    "        \n",
    "        validsplit = []\n",
    "        for i in range(0,len(valid),self.batchsize):\n",
    "             validsplit.append(valid[i:i + self.batchsize])\n",
    "                \n",
    "        trainsplit = []\n",
    "        for i in range(0,len(train),self.batchsize):\n",
    "             trainsplit.append(train[i:i + self.batchsize])\n",
    "        \n",
    "        return trainsplit, validsplit\n",
    "        \n",
    "    \n",
    "    def train_next(self):\n",
    "        try:\n",
    "            paths = self.trainsplit[self.batchnumber]\n",
    "            pictures = self._get_pictures(paths)\n",
    "            targets = self.onehot.loc[paths].values\n",
    "        \n",
    "            self.batchnumber += 1\n",
    "        \n",
    "            return pictures, targets\n",
    "        \n",
    "        except:\n",
    "            self.batchnumber = 0\n",
    "            return None, None\n",
    "    \n",
    "    def valid_next(self):\n",
    "        try:\n",
    "            paths = self.validsplit[self.batchnumber]\n",
    "            pictures = self._get_pictures(paths)\n",
    "            targets = self.onehot.loc[paths].values\n",
    "        \n",
    "            self.batchnumber += 1\n",
    "        \n",
    "            return pictures, targets\n",
    "        \n",
    "        except:\n",
    "            self.batchnumber = 0\n",
    "            return None, None\n",
    "        \n",
    "    \n",
    "    def print_classes(self):\n",
    "        \n",
    "        for i in range(len(self.classlist)):\n",
    "            print(f\"Class: {i} - {self.countlist[i]}\\t\\t - {self.classlist[i]}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_precision(pred, true):\n",
    "    predicted_positives = (np.array(pred) == 1).sum()\n",
    "    \n",
    "    true_positives = np.array([a == b and a == 1 for a, b in zip(pred, true)])\n",
    "    true_positives = (np.array(true_positives) == 1).sum()\n",
    "\n",
    "    result = true_positives / predicted_positives\n",
    "    \n",
    "    return result\n",
    "\n",
    "def AP(pred, true):\n",
    "    precision = calculate_precision(pred, true)\n",
    "    a = np.array([a == b and a == 1 for a, b in zip(pred, true)])\n",
    "    score = (np.array(a) ==True).sum() * precision\n",
    "    \n",
    "    result = score / K\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def MAP(predicted_data, true_data):\n",
    "    result = np.array([AP(pred, true) for pred, true in zip(predicted_data, true_data)]).sum() / K\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(batch=None,labels=None):\n",
    "    if batch != None:\n",
    "        # prepare data\n",
    "        batch = torch.tensor(batch).float()\n",
    "        labels = torch.tensor(labels).float()\n",
    "        device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "        batch = batch.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        return batch,labels\n",
    "    else:\n",
    "        return none,none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model_conv = models.resnet50(pretrained='imagenet')\n",
    "\n",
    "# Disable autograd for resnet\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Change fully connected layer to match paper (autograd is default on new layers)\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 1024)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_conv = model_conv.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classfolder=\"./dataset_v2/root/train/classes/\"\n",
    "croppedfolder = \"./dataset_v2/root/train/cropped/\"\n",
    "picturesize = 224\n",
    "batchsize = 64\n",
    "split_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainloader = Trainloader(classfolder,croppedfolder,picturesize,batchsize,split_ratio)\n",
    "flattened = [val for sublist in trainloader.validsplit for val in sublist]\n",
    "\n",
    "dataset_positives = 0.0\n",
    "for column in trainloader.onehot.loc[flattened].columns:\n",
    "    dataset_positives += trainloader.onehot.loc[trainloader.onehot[column] == 1].sum()[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        output_nodes = trainloader.onehot.shape[1]\n",
    "        self.output = nn.Linear(1024,output_nodes)\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = model_conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "epochs = 100\n",
    "threshold = 0.5\n",
    "K = len(trainloader.classlist)\n",
    "valid_every = 2\n",
    "#criterion = nn.BCELoss()\n",
    "#criterion = Custom_Loss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9) \n",
    "\n",
    "losses_train = pd.DataFrame(columns=['Epoch','Loss'])\n",
    "losses_valid = pd.DataFrame(columns=['Epoch','Loss'])\n",
    "\n",
    "accuracy = 0.01 #dummy value\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch_loss = []\n",
    "    batch,labels = trainloader.train_next()\n",
    "    net.train()\n",
    "    \n",
    "    while batch != None:\n",
    "        # Prepare data\n",
    "        batch,labels = prepare_data(batch,labels)\n",
    "            \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # Make prediction and backpropogate\n",
    "        output = net(batch)\n",
    "        loss = criterion(output,labels)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss.append(loss.item())\n",
    "        \n",
    "        # Load next batch\n",
    "        batch,labels = trainloader.train_next()\n",
    "        \n",
    "    # calculate the loss of the training set   \n",
    "    losses_train.loc[epoch] = [epoch+1,np.mean(batch_loss)]\n",
    "    \n",
    "    if epoch%valid_every == 0 or epoch == max(range(epochs)):\n",
    "        batch_loss = []\n",
    "        preds = []\n",
    "        trues = []\n",
    "        batch,labels = trainloader.valid_next()\n",
    "        net.eval()\n",
    "        \n",
    "        while batch != None:\n",
    "            # Prepare data\n",
    "            batch,labels = prepare_data(batch,labels)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Make prediction\n",
    "            output = net(batch)\n",
    "            loss = criterion(output,labels)\n",
    "            batch_loss.append(loss.item())\n",
    "            \n",
    "            # Append values for later calculation of accuracy\n",
    "            preds.append([[1 if pred > threshold else 0 for pred in sample] for sample in output])\n",
    "            trues.append([[int(value) for value in valuelist] for valuelist in labels.tolist()])\n",
    "            \n",
    "            # Load next batch\n",
    "            batch,labels = trainloader.valid_next()\n",
    "        \n",
    "        # Calculate loss and accuracy for the validation set    \n",
    "        losses_valid.loc[epoch] = [epoch+1,np.mean(batch_loss)]\n",
    "        accuracy = MAP(preds[0], trues[0])\n",
    "    \n",
    "    # Plot the loss every epoch\n",
    "    pl.figure(figsize=(15,5))\n",
    "    pl.xlabel('Epochs')\n",
    "    pl.ylabel('Loss')\n",
    "    pl.title('Epoch #{}'.format(epoch+1))\n",
    "    pl.plot(losses_train['Epoch'],losses_train['Loss'], '-b', label='Train')\n",
    "    pl.plot(losses_valid['Epoch'],losses_valid['Loss'], '-r', label='Valid')\n",
    "    pl.legend()\n",
    "    pl.show()\n",
    "    print('Training loss: {:.2f}'.format(losses_train['Loss'].iloc[-1],))\n",
    "    print('Validation loss:{:.2f}\\tValidation MAP: {:.2f}'.format(losses_valid['Loss'].iloc[-1],accuracy))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "\n",
    "# Final plot    \n",
    "pl.figure(figsize=(15,5))\n",
    "pl.xlabel('Epochs')\n",
    "pl.ylabel('Loss')\n",
    "pl.title('Epoch #{}'.format(epoch+1))\n",
    "pl.plot(losses_train['Epoch'],losses_train['Loss'], '-b', label='Train')\n",
    "pl.plot(losses_valid['Epoch'],losses_valid['Loss'], '-r', label='Valid')\n",
    "pl.legend()\n",
    "pl.show()\n",
    "\n",
    "print('Training loss: {:.2f}'.format(losses_train['Loss'].iloc[-1],))\n",
    "print('Validation loss:{:.2f}\\tValidation MAP: {:.2f}'.format(losses_valid['Loss'].iloc[-1],accuracy))\n",
    "\n",
    "print(\"\\n\\t\\t\\t\\t\\tTraining time {}\".format(datetime.datetime.now()-now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pl.figure(figsize=(15,5))\n",
    "pl.xlabel('Epochs')\n",
    "pl.ylabel('Loss')\n",
    "pl.title('Epoch #{}'.format(epoch+1))\n",
    "pl.plot(losses_train['Epoch'],losses_train['Loss'], '-b', label='Train')\n",
    "pl.plot(losses_valid['Epoch'],losses_valid['Loss'], '-r', label='Valid')\n",
    "pl.legend()\n",
    "pl.show()\n",
    "\n",
    "print('Training loss: {:.2f}'.format(losses_train['Loss'].iloc[-1],))\n",
    "print('Validation loss:{:.2f}\\tValidation MAP: {:.2f}'.format(losses_valid['Loss'].iloc[-1],accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch,labels = trainloader.valid_next()\n",
    "batch,labels = prepare_data(batch,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion(output,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Loss(nn.modules.Module):\n",
    "    def __init__(self):\n",
    "        super(Custom_Loss, self).__init__()\n",
    "        self.N = trainloader.onehot.shape[0]\n",
    "        self.Nm = np.array(trainloader.countlist)\n",
    "        self.W0 = [max( x /self.N,0.1) for x in self.Nm]\n",
    "        self.W = [(1-w0)/w0 for w0 in self.W0]\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        N = input.shape[0]\n",
    "        M = target.shape[1]\n",
    "        T = torch.tensor(self.W)\n",
    "        T = T.type(output.type())\n",
    "        result = torch.tensor(0)\n",
    "        result = result.type(output.type())\n",
    "        \n",
    "        for n in range(1,N):\n",
    "            result += sum(T*target[n]*torch.log10(input[n])+(1-target[n])*torch.log10(1-input[n]))\n",
    "                \n",
    "        result = (1/N*M)*result\n",
    "       \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = Custom_Loss()\n",
    "criterion(output,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W*labels[0]\n",
    "output_test = output.clone()\n",
    "T = torch.tensor(W)\n",
    "T = T.type(output.type())\n",
    "T = sum(T*labels[0]*torch.log10(output_test[0])+(1-labels[0])*torch.log10(1-output_test[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log10(output_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(output_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = W[0]*labels[0][0]*m.log10(output[0][0])+(1-labels[0][0])*m.log10(1-output[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listen = trainloader.countlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array(trainloader.countlist)\n",
    "array.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = [max( x /11617,0.1) for x in array]\n",
    "W = [(1-w0)/w0 for w0 in scale]\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader.onehot.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.W[n]*target[n]*m.log10(input[n])+(1-target[n])*m.log10(1-input(n))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
