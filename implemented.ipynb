{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nerual Network for Classifying fine-grained images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Operating system: darwin\n",
      "Using pytorch backend\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "from libcpab.libcpab.pytorch import cpab\n",
    "from libcpab.libcpab.helper.utility import show_images\n",
    "from IPython.display import display\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from skimage import io\n",
    "from skimage.transform import rescale\n",
    "import datetime\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Loss(nn.modules.Module):\n",
    "    def __init__(self):\n",
    "        super(Custom_Loss, self).__init__()\n",
    "        self.N = trainloader.onehot.shape[0]\n",
    "        self.Nm = np.array(trainloader.countlist)\n",
    "        self.W0 = [max( x /self.N,0.1) for x in self.Nm]\n",
    "        self.W = [(1-w0)/w0 for w0 in self.W0]\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        N = input.shape[0]\n",
    "        M = target.shape[1]\n",
    "        T = torch.tensor(self.W)\n",
    "        T = T.type(input.type())\n",
    "        result = torch.tensor(0)\n",
    "        result = result.type(output.type())\n",
    "        \n",
    "        for n in range(N):\n",
    "            #result += sum((T*target[n]*torch.log10(abs(input[n])+1e-16)+(1-target[n])*torch.log10(1-abs(input[n]))))\n",
    "            result += -1*sum(T*target[n]*(torch.log_softmax(input[n],dim=0))+(1-target[n])*(torch.log_softmax(1-input[n],dim=0)))\n",
    "                \n",
    "        result = (1/(N*M))*result\n",
    "       \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(pred, true):\n",
    "    predicted_positives = (np.array(pred) == 1).sum()\n",
    "    \n",
    "    true_positives = np.array([a == b and a == 1 for a, b in zip(pred, true)])\n",
    "    true_positives = (np.array(true_positives) == 1).sum()\n",
    "\n",
    "    result = true_positives / predicted_positives\n",
    "    \n",
    "    return result\n",
    "\n",
    "def AP(pred, true):\n",
    "    precision = calculate_precision(pred, true)\n",
    "    a = np.array([a == b and a == 1 for a, b in zip(pred, true)])\n",
    "    score = (np.array(a) ==True).sum() * precision\n",
    "    \n",
    "    result = score / K\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def MAP(predicted_data, true_data):\n",
    "    result = np.array([AP(pred, true) for pred, true in zip(predicted_data, true_data)]).sum() / K\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(path=None, samples=None, DIM=224, optimize=False, N_fixed=None, N_opt=2, show_result=False):\n",
    "    \"\"\"\n",
    "    Upsample images with CPAB augmentation technique.\n",
    "    Augmented images are saved in same directory as original images.\n",
    "\n",
    "        Args\n",
    "            path: Path to directory with images.\n",
    "            samples: Dictionary with ID's and corresponding upsampling size. Ex: {'ID_1': 10}\n",
    "            DIM: Dimension of agumented images. (integer as H x W are the same)\n",
    "            optimize: If true the augmentation is optimized with the Adam optimizer,\n",
    "                      and the learned estimation of the transformed data is returned\n",
    "                      instead of a first transformation. Default false.\n",
    "            N_fixed: Number of augmented samples to return, should be give in samples dict {'sample_id': N}.\n",
    "            N_opt: Number of times optimization procedure should be run, default 2.\n",
    "            show_result: If true transformed images are presented, default false.\n",
    "    \"\"\"\n",
    "\n",
    "    if not path:\n",
    "        print(\"Please specify path to directory with samples.\")\n",
    "        return 0\n",
    "\n",
    "    if not samples:\n",
    "        print(\"No dictionary with sample ID's and sample size given. See _calculate_augmentation()\")\n",
    "        return 0\n",
    "\n",
    "    # convert images from PNG to JPG\n",
    "    for sample in samples:\n",
    "        # number of times sample should be upsampled\n",
    "        N = samples.get(sample)\n",
    "        if N_fixed != None:\n",
    "            N = N_fixed\n",
    "\n",
    "        # read image and convert from RGBA to RGB\n",
    "        im = Image.open(path+sample)\n",
    "\n",
    "        # convert images from RGBA to RGB\n",
    "        if im.mode == \"RGBA\":\n",
    "            im.load() # required for im.split()\n",
    "            newIm = Image.new(\"RGB\", im.size, color=(0, 0, 0)) # black - RGB(0,0,0) - background color\n",
    "            newIm.paste(im, mask=im.split()[3]) # 3 is the alpha channel\n",
    "            newIm.save(path+sample, \"PNG\", quality=80) # save image with old file ending name\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # augment images by estimation of the transformed data\n",
    "        if optimize:\n",
    "            for n in range(N):\n",
    "                # read image again as RGB with 3 channels and normalize between [0, 1]\n",
    "                data = plt.imread(path+sample, format=\"RGB\") / 255\n",
    "                data = np.expand_dims(data, 0) # create batch of data\n",
    "                # Convert to torch tensor and torch format [n_batch, n_channels, width, height]\n",
    "                data = torch.Tensor(data).permute(0,3,1,2)\n",
    "\n",
    "                # Define transformer class\n",
    "                T1 = cpab(tess_size=[3,3], device='cpu')\n",
    "\n",
    "                # Sample random transformation\n",
    "                theta_true = 0.5*T1.sample_transformation(1)\n",
    "\n",
    "                # Transform the images\n",
    "                transformed_data = T1.transform_data(data, theta_true, outsize=(DIM,DIM))\n",
    "\n",
    "                # define a PyTorch procedure that enables estimation of the transformation\n",
    "                T2 = cpab(tess_size=[3,3], device='cpu')\n",
    "                theta_est = T2.identity(1, epsilon=1e-4)\n",
    "                theta_est.requires_grad = True\n",
    "\n",
    "                # PyTorch optimizer for estimation procedure\n",
    "                optimizer = torch.optim.Adam([theta_est], lr=0.1)\n",
    "\n",
    "                # Optimization loop\n",
    "                max_opt = N_opt\n",
    "                for i in range(max_opt):\n",
    "                    trans_est = T2.transform_data(data, theta_est, outsize=(DIM, DIM))\n",
    "                    loss = (transformed_data.to(trans_est.device) - trans_est).pow(2).mean()\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                if show_result:\n",
    "                    plt.subplots(1,3, figsize=(10, 15))\n",
    "                    plt.subplot(1,3,1)\n",
    "                    plt.imshow(data.permute(0,2,3,1).numpy()[0])\n",
    "                    plt.axis('off')\n",
    "                    plt.title('Source')\n",
    "                    plt.subplot(1,3,2)\n",
    "                    plt.imshow(transformed_data.permute(0,2,3,1).cpu().numpy()[0])\n",
    "                    plt.axis('off')\n",
    "                    plt.title('Target')\n",
    "                    plt.subplot(1,3,3)\n",
    "                    plt.imshow(trans_est.permute(0,2,3,1).cpu().detach().numpy()[0])\n",
    "                    plt.axis('off')\n",
    "                    plt.title('Estimate')\n",
    "                    plt.show()\n",
    "\n",
    "                # Get the corresponding numpy arrays in correct format [n_batch, width, height, n_channels]\n",
    "                transformed_data = trans_est.permute(0, 2, 3, 1).cpu().detach().numpy()[0]\n",
    "\n",
    "                # save augmented versions\n",
    "                # name of image: aug[x]_[original_name] where x is in the interval [0,N]\n",
    "                im = transformed_data * 255 # normalize pixel values between [0,255]\n",
    "                filename = \"aug\"+str(n)+\"_\"+sample\n",
    "                image = Image.fromarray(im.astype('uint8'), mode=\"RGB\")\n",
    "                image.save(os.path.join(path, filename)) # save in class folder\n",
    "\n",
    "        # augment images without transformation optimization\n",
    "        else:\n",
    "            # read image again as RGB with 3 channels and normalize between [0, 1]\n",
    "            data = plt.imread(path+sample, format=\"RGB\") / 255\n",
    "            data = np.tile(np.expand_dims(data, 0), [N,1,1,1]) # create batch of data\n",
    "            # Convert to torch tensor and torch format [n_batch, n_channels, width, height]\n",
    "            data = torch.Tensor(data).permute(0,3,1,2)\n",
    "\n",
    "            # Define transformer class\n",
    "            T1 = cpab(tess_size=[3,3], device='cpu')\n",
    "\n",
    "            # Sample random transformation\n",
    "            theta_true = 0.5*T1.sample_transformation(N)\n",
    "\n",
    "            # Transform the images\n",
    "            transformed_data = T1.transform_data(data, theta_true, outsize=(DIM,DIM))\n",
    "\n",
    "            # Get the corresponding numpy arrays in correct format [n_batch, width, height, n_channels]\n",
    "            transformed_data = transformed_data.permute(0, 2, 3, 1).cpu().numpy()\n",
    "\n",
    "            if show_result:\n",
    "                show_images(transformed_data)\n",
    "\n",
    "            # save augmented versions\n",
    "            # name of image: aug[x]_[original_name] where x is in the interval [0,N]\n",
    "            n = 0\n",
    "            for data_sample in transformed_data:\n",
    "                im = data_sample * 255 # normalize pixel values between [0,255]\n",
    "                filename = \"aug\"+str(n)+\"_\"+sample\n",
    "                image = Image.fromarray(im.astype('uint8'), mode=\"RGB\")\n",
    "                image.save(os.path.join(path, filename)) # save in class folder\n",
    "                n += 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_augmentation(df, criteria=1000, restriction=10):\n",
    "    \"\"\"\n",
    "    Calculate how many times a sample is to be upsampled with augmentation\n",
    "    compared to the represantation of its class or if multilabel, its classes.\n",
    "\n",
    "    Be aware of samples of a low represented class can be part of a larger class also.\n",
    "    The aforementioned can result in upsampling of high represented classes as well,\n",
    "    when upsampling low represented classes.\n",
    "\n",
    "    Args\n",
    "        df: A dataframe which the upsampling size is calculated from.\n",
    "            The indexes should be the ID's of the samples.\n",
    "        criteria: The criteria to match number of times to upsample a class with.\n",
    "    \"\"\"\n",
    "    # placeholders\n",
    "    classes = {}\n",
    "    samplesToAugment = {}\n",
    "\n",
    "    # get number of occurrences for each class\n",
    "    for column in df.columns:\n",
    "        num_occ = len(df.loc[df[column] == 1])\n",
    "        classes[column] = num_occ\n",
    "\n",
    "    # sort classes by ascending size\n",
    "    classes_sorted = sorted(classes.items(), key=lambda value: value[1])\n",
    "\n",
    "    # iterate over classes and calculate\n",
    "    # number of times to augment data to\n",
    "    # match specified criteria\n",
    "    for cls in classes_sorted:\n",
    "        # get indexes for samples where class value is 1\n",
    "        indexesInClass = df.loc[df[cls[0]] == 1].index\n",
    "\n",
    "        # remove indexes that already are appended to samplesToAugment\n",
    "        indexesToAugment = [indexesInClass[i] for i in range(len(indexesInClass)) \\\n",
    "                            if indexesInClass[i] not in samplesToAugment.keys()]\n",
    "\n",
    "        # find how many times already is upsampled\n",
    "        already_size = sum([samplesToAugment.get(sample) for sample in indexesInClass \\\n",
    "                            if sample in samplesToAugment.keys() and sample not in indexesToAugment])\n",
    "\n",
    "        # calculate number of times samples in class \n",
    "        # should be upsampled to match specified criteria\n",
    "        if len(indexesToAugment) == 0:\n",
    "            augment_size = int(1000 - already_size)\n",
    "        # split upsample size over all samples to augment\n",
    "        else:\n",
    "            augment_size = int((1000-already_size) / len(indexesToAugment))\n",
    "\n",
    "        # respect restriction criteria\n",
    "        if augment_size > restriction:\n",
    "            augment_size = 10\n",
    "        # if class is already upsampled enough\n",
    "        # do not upsample remaining samples in class\n",
    "        elif augment_size < 0:\n",
    "            augment_size = 0\n",
    "\n",
    "        # save indexes to augment and corresponding number of times to upsample\n",
    "        for index in indexesToAugment:\n",
    "            if index in samplesToAugment.keys():\n",
    "                print(\"Index already in samplesToAugment, overwriting key..\")\n",
    "            samplesToAugment[index] = augment_size\n",
    "\n",
    "    return samplesToAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_augmented(path=None, search_string='aug'):\n",
    "    \"\"\"\n",
    "    Designed for appending augmented data to original dataframe.\n",
    "    Append data to a dataframe where appended data gets its values from\n",
    "    its sample of origin in the dataframe. \n",
    "\n",
    "    Args\n",
    "        df: Pandas dataframe which data is to be appended to.\n",
    "        path: Path to directory with augmented files.\n",
    "        search_string: A regex string criteria for searching for files in directory.\n",
    "    \"\"\"\n",
    "    if not path:\n",
    "        print(\"Specify the path to augmented files.\")\n",
    "        return 0\n",
    "\n",
    "    # get filenames for augmented images\n",
    "    filenames = [name for name in os.listdir(path) if os.path.isfile(path+name) and search_string in name]\n",
    "\n",
    "    # construct temporary dataframe for data to append\n",
    "    df_append = pd.DataFrame(data=[name for name in filenames], columns=['id']).set_index(keys='id')\n",
    "\n",
    "    # iterate over all row indexes in dataframe to append\n",
    "    for i in df_append.index:\n",
    "        # get original target values for original image for augmented image\n",
    "        values = df.iloc[list(np.where(df.index == i.replace('aug_', ''))[0])].values\n",
    "\n",
    "        # iterate over columns and corresponding values in dataframe to append to\n",
    "        for column, value in zip(range(len(df.columns)), values[0]):\n",
    "            df_append[f\"{df.columns[column]}\"] = value\n",
    "\n",
    "    df_new = pd.concat([self.df, df_append])\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(df, trainsplit_size=0.8):\n",
    "    \"\"\"\n",
    "    Split a dataset in training- and validation split.\n",
    "\n",
    "    Split is made by splitting the least represented class first\n",
    "    and the highest represented class last.\n",
    "\n",
    "    Function is made for multilabel problems where samples that are already \n",
    "    drawn from a previous class are not drawn again, and the ratio of the \n",
    "    split is calculated at each class from the previous draws to match the \n",
    "    split criteria.\n",
    "\n",
    "    Args\n",
    "        df: A dataframe which the split should be created from.\n",
    "            The indexes should be the ID's of the samples.\n",
    "        trainsplit_size: The size of the training split. \n",
    "                         Should be a float in the interval [0, 1]\n",
    "    \"\"\"\n",
    "    # placeholders\n",
    "    cls = {}\n",
    "    train_idx = []\n",
    "    valid_idx = []\n",
    "\n",
    "    # get number of occurrences for each class\n",
    "    for col in df.columns:\n",
    "        size = len(df.loc[df[col] == 1])\n",
    "        cls[col] = size\n",
    "\n",
    "    # sort classes by ascending\n",
    "    cls_sorted = sorted(cls.items(), key=lambda value: value[1])\n",
    "\n",
    "    # iterate over classes from least represented class\n",
    "    # and draw samples for training- and validation split\n",
    "    for col in cls_sorted:\n",
    "        # get indexes for samples where class value is 1\n",
    "        indexes = df.loc[df[col[0]] == 1].index\n",
    "\n",
    "        # Remove indexes that already are appended to test_idx array\n",
    "        indexes = [indexes[i] for i in range(len(indexes)) if indexes[i] not in valid_idx and indexes[i] not in train_idx]\n",
    "\n",
    "        # get size of how many indexes should be drawn for train\n",
    "        train_size = int(len(indexes) * trainsplit_size)\n",
    "\n",
    "        # get indexes for train- and validation split\n",
    "        idx_train = [indexes[i] for i in sorted(random.sample(range(len(indexes)), train_size))]\n",
    "        idx_valid = [indexes[i] for i in range(len(indexes)) if indexes[i] not in idx_train]\n",
    "\n",
    "        # save indexes\n",
    "        train_idx.extend(idx_train)\n",
    "        valid_idx.extend(idx_valid)\n",
    "\n",
    "    return train_idx, valid_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainloader:\n",
    "    \n",
    "    def __init__(self, croppedfolder, picturesize, batchsize, csv_path, trainsplit_size):\n",
    "        self.cropdirectory = croppedfolder\n",
    "        self.picturesize = picturesize\n",
    "        self.batchsize = batchsize\n",
    "        self.onehot = self._get_onehot(csv_path)\n",
    "        self.trainsplit, self.validsplit = self.train_valid_split(self.onehot, trainsplit_size)\n",
    "        self.batch = 0\n",
    "        \n",
    "    \n",
    "    def _get_onehot(self, csv_path):\n",
    "        \"\"\"\n",
    "        Construct dataframe.\n",
    "\n",
    "        Args\n",
    "            csv_path: Path to csv file.\n",
    "        \"\"\"\n",
    "        # construct dataframe with onehot notation\n",
    "        df = pd.get_dummies(pd.read_csv(csv_path))\n",
    "        # create ID column with image file names\n",
    "        df['id'] = df['image_id'].apply(lambda x: str(x)) + \"_\" + df['tag_id'].apply(lambda x: str(x)) + \".png\"\n",
    "        # set ID column to index in dataframe\n",
    "        df = df.set_index(keys='id')\n",
    "        # only use class columns\n",
    "        df = df.iloc[:, 10:]\n",
    "        return df\n",
    "    \n",
    "    def train_valid_split(self, df, trainsplit_size=0.8):\n",
    "        \"\"\"\n",
    "        Split a dataset in training- and validation split.\n",
    "\n",
    "        Split is made by splitting the least represented class first\n",
    "        and the highest represented class last.\n",
    "\n",
    "        Function is made for multilabel problems where samples that are already \n",
    "        drawn from a previous class are not drawn again, and the ratio of the \n",
    "        split is calculated at each class from the previous draws to match the \n",
    "        split criteria.\n",
    "\n",
    "        Args\n",
    "            df: A dataframe which the split should be created from.\n",
    "                The indexes should be the ID's of the samples.\n",
    "            trainsplit_size: The size of the training split. \n",
    "                             Should be a float in the interval [0, 1]\n",
    "        \"\"\"\n",
    "        # placeholders\n",
    "        cls = {}\n",
    "        train_idx = []\n",
    "        valid_idx = []\n",
    "\n",
    "        # get number of occurrences for each class\n",
    "        for col in df.columns:\n",
    "            size = len(df.loc[df[col] == 1])\n",
    "            cls[col] = size\n",
    "\n",
    "        # sort classes by ascending\n",
    "        cls_sorted = sorted(cls.items(), key=lambda value: value[1])\n",
    "\n",
    "        # iterate over classes from least represented class\n",
    "        # and draw samples for training- and validation split\n",
    "        for col in cls_sorted:\n",
    "            # get indexes for samples where class value is 1\n",
    "            indexes = df.loc[df[col[0]] == 1].index\n",
    "\n",
    "            # Remove indexes that already are appended to test_idx array\n",
    "            indexes = [indexes[i] for i in range(len(indexes)) if indexes[i] not in valid_idx and indexes[i] not in train_idx]\n",
    "\n",
    "            # get size of how many indexes should be drawn for train\n",
    "            train_size = int(len(indexes) * trainsplit_size)\n",
    "\n",
    "            # get indexes for train- and validation split\n",
    "            idx_train = [indexes[i] for i in sorted(random.sample(range(len(indexes)), train_size))]\n",
    "            idx_valid = [indexes[i] for i in range(len(indexes)) if indexes[i] not in idx_train]\n",
    "\n",
    "            # save indexes\n",
    "            train_idx.extend(idx_train)\n",
    "            valid_idx.extend(idx_valid)\n",
    "\n",
    "        return train_idx, valid_idx\n",
    "\n",
    "    def _convert_picture(self, path):\n",
    "        image = io.imread(path)\n",
    "        rescale_factor = self.picturesize / image.shape[0],self.picturesize / image.shape[1]\n",
    "        scaled = rescale(image, rescale_factor, anti_aliasing=False, multichannel=True,mode='constant')[:,:,:3]\n",
    "        scaled = scaled.transpose((-1, 0, 1))\n",
    "\n",
    "        return scaled\n",
    "    \n",
    "    def _get_pictures(self, paths):\n",
    "        trainlist = []\n",
    "        for path in paths:\n",
    "            trainlist.append(self._convert_picture(self.cropdirectory+path))\n",
    "            \n",
    "        return trainlist\n",
    "    \n",
    "    def train_next(self):\n",
    "        try:\n",
    "            paths = self.trainsplit[self.batch:self.batchsize]\n",
    "            pictures = self._get_pictures(paths)\n",
    "            targets = self.onehot.loc[paths].values\n",
    "        \n",
    "            self.batch += self.batchsize\n",
    "        \n",
    "            return pictures, targets\n",
    "        \n",
    "        except:\n",
    "            self.batch = 0\n",
    "            return None, None\n",
    "    \n",
    "    def valid_next(self):\n",
    "        try:\n",
    "            paths = self.validsplit[self.batch:self.batchsize]\n",
    "            pictures = self._get_pictures(paths)\n",
    "            targets = self.onehot.loc[paths].values\n",
    "        \n",
    "            self.batch += self.batchsize\n",
    "        \n",
    "            return pictures, targets\n",
    "        \n",
    "        except:\n",
    "            self.batch = 0\n",
    "            return None, None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(batch=None,labels=None):\n",
    "    if batch != None:\n",
    "        # prepare data\n",
    "        batch = torch.tensor(batch).float()\n",
    "        labels = torch.tensor(labels).float()\n",
    "        device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "        batch = batch.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        return batch,labels\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "croppedfolder = \"/Users/mattias/Desktop/cropped/\"\n",
    "csv_path = \"./data/dataset_v2/train.csv\"\n",
    "picturesize = 224\n",
    "batchsize = 64\n",
    "split_ratio = 0.8 # size of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = Trainloader(croppedfolder,picturesize,batchsize,csv_path,split_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find number of times each sample should be upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_augmentation(df, criteria=1000, restriction=10):\n",
    "    \"\"\"\n",
    "    Calculate how many times a sample is to be upsampled with augmentation\n",
    "    compared to the represantation of its class or if multilabel, its classes.\n",
    "\n",
    "    Be aware of samples of a low represented class can be part of a larger class also.\n",
    "    The aforementioned can result in upsampling of high represented classes as well,\n",
    "    when upsampling low represented classes.\n",
    "\n",
    "    Args\n",
    "        df: A dataframe which the upsampling size is calculated from.\n",
    "            The indexes should be the ID's of the samples.\n",
    "        criteria: The criteria to match number of times to upsample a class with.\n",
    "    \"\"\"\n",
    "    # placeholders\n",
    "    classes = {}\n",
    "    samplesToAugment = {}\n",
    "\n",
    "    # get number of occurrences for each class\n",
    "    for column in df.columns:\n",
    "        num_occ = len(df.loc[df[column] == 1])\n",
    "        classes[column] = num_occ\n",
    "\n",
    "    # sort classes by ascending size\n",
    "    classes_sorted = sorted(classes.items(), key=lambda value: value[1])\n",
    "\n",
    "    # iterate over classes and calculate\n",
    "    # number of times to augment data to\n",
    "    # match specified criteria\n",
    "    for cls in classes_sorted:\n",
    "        # get indexes for samples where class value is 1\n",
    "        indexesInClass = df.loc[df[cls[0]] == 1].index\n",
    "\n",
    "        # remove indexes that already are appended to samplesToAugment\n",
    "        indexesToAugment = [indexesInClass[i] for i in range(len(indexesInClass)) \\\n",
    "                            if indexesInClass[i] not in samplesToAugment.keys()]\n",
    "\n",
    "        # find how many times already is upsampled\n",
    "        already_size = sum([samplesToAugment.get(sample) for sample in indexesInClass \\\n",
    "                            if sample in samplesToAugment.keys() and sample not in indexesToAugment])\n",
    "\n",
    "        # calculate number of times samples in class \n",
    "        # should be upsampled to match specified criteria\n",
    "        if len(indexesToAugment) == 0:\n",
    "            augment_size = int(1000 - already_size)\n",
    "        # split upsample size over all samples to augment\n",
    "        else:\n",
    "            augment_size = int((1000-already_size) / len(indexesToAugment))\n",
    "\n",
    "        # respect restriction criteria\n",
    "        if augment_size > restriction:\n",
    "            augment_size = 10\n",
    "        # if class is already upsampled enough\n",
    "        # do not upsample remaining samples in class\n",
    "        elif augment_size < 0:\n",
    "            augment_size = 0\n",
    "\n",
    "        # save indexes to augment and corresponding number of times to upsample\n",
    "        for index in indexesToAugment:\n",
    "            if index in samplesToAugment.keys():\n",
    "                print(\"Index already in samplesToAugment, overwriting key..\")\n",
    "            samplesToAugment[index] = augment_size\n",
    "\n",
    "    return samplesToAugment\n",
    "\n",
    "samples_augment = calculate_augmentation(df=trainloader.onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsample the dataset using augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-42ef4aa5c15a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mupsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcroppedfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msamples_augment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-647fd492d0db>\u001b[0m in \u001b[0;36mupsample\u001b[0;34m(path, samples, DIM, optimize, N_fixed, N_opt, show_result)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# create batch of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m# Convert to torch tensor and torch format [n_batch, n_channels, width, height]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;31m# Define transformer class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    "upsample(path=croppedfolder, samples=samples_augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append the augmented samples to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug = get_onehot_augmented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model_conv = models.resnet50(pretrained='imagenet')\n",
    "\n",
    "# Disable autograd for resnet\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Change fully connected layer to match paper (autograd is default on new layers)\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 1024)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_conv = model_conv.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsplit, validsplit = train_valid_split()\n",
    "trainloader = Trainloader(classfolder,croppedfolder,picturesize,batchsize)\n",
    "flattened = [val for sublist in trainloader.validsplit for val in sublist]\n",
    "\n",
    "dataset_positives = 0.0\n",
    "for column in trainloader.onehot.loc[flattened].columns:\n",
    "    dataset_positives += trainloader.onehot.loc[trainloader.onehot[column] == 1].sum()[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        output_nodes = trainloader.onehot.shape[1]\n",
    "        self.output = nn.Linear(1024,output_nodes)\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = model_conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "epochs = 100\n",
    "threshold = 0.5\n",
    "K = len(trainloader.classlist)\n",
    "valid_every = 2\n",
    "#criterion = nn.BCELoss()\n",
    "#criterion = Custom_Loss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9) \n",
    "\n",
    "losses_train = pd.DataFrame(columns=['Epoch','Loss'])\n",
    "losses_valid = pd.DataFrame(columns=['Epoch','Loss'])\n",
    "\n",
    "accuracy = 0.01 #dummy value\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch_loss = []\n",
    "    batch,labels = trainloader.train_next()\n",
    "    net.train()\n",
    "    \n",
    "    while batch != None:\n",
    "        # Prepare data\n",
    "        batch,labels = prepare_data(batch,labels)\n",
    "            \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # Make prediction and backpropogate\n",
    "        output = net(batch)\n",
    "        loss = criterion(output,labels)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss.append(loss.item())\n",
    "        \n",
    "        # Load next batch\n",
    "        batch,labels = trainloader.train_next()\n",
    "        \n",
    "    # calculate the loss of the training set   \n",
    "    losses_train.loc[epoch] = [epoch+1,np.mean(batch_loss)]\n",
    "    \n",
    "    if epoch%valid_every == 0 or epoch == max(range(epochs)):\n",
    "        batch_loss = []\n",
    "        preds = []\n",
    "        trues = []\n",
    "        batch,labels = trainloader.valid_next()\n",
    "        net.eval()\n",
    "        \n",
    "        while batch != None:\n",
    "            # Prepare data\n",
    "            batch,labels = prepare_data(batch,labels)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Make prediction\n",
    "            output = net(batch)\n",
    "            loss = criterion(output,labels)\n",
    "            batch_loss.append(loss.item())\n",
    "            \n",
    "            # Append values for later calculation of accuracy\n",
    "            preds.append([[1 if pred > threshold else 0 for pred in sample] for sample in output])\n",
    "            trues.append([[int(value) for value in valuelist] for valuelist in labels.tolist()])\n",
    "            \n",
    "            # Load next batch\n",
    "            batch,labels = trainloader.valid_next()\n",
    "        \n",
    "        # Calculate loss and accuracy for the validation set    \n",
    "        losses_valid.loc[epoch] = [epoch+1,np.mean(batch_loss)]\n",
    "        accuracy = MAP(preds[0], trues[0])\n",
    "    \n",
    "    # Plot the loss every epoch\n",
    "    pl.figure(figsize=(15,5))\n",
    "    pl.xlabel('Epochs')\n",
    "    pl.ylabel('Loss')\n",
    "    pl.title('Epoch #{}'.format(epoch+1))\n",
    "    pl.plot(losses_train['Epoch'],losses_train['Loss'], '-b', label='Train')\n",
    "    pl.plot(losses_valid['Epoch'],losses_valid['Loss'], '-r', label='Valid')\n",
    "    pl.legend()\n",
    "    pl.show()\n",
    "    print('Training loss: {:.2f}'.format(losses_train['Loss'].iloc[-1],))\n",
    "    print('Validation loss:{:.2f}\\tValidation MAP: {:.2f}'.format(losses_valid['Loss'].iloc[-1],accuracy))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "\n",
    "# Final plot    \n",
    "pl.figure(figsize=(15,5))\n",
    "pl.xlabel('Epochs')\n",
    "pl.ylabel('Loss')\n",
    "pl.title('Epoch #{}'.format(epoch+1))\n",
    "pl.plot(losses_train['Epoch'],losses_train['Loss'], '-b', label='Train')\n",
    "pl.plot(losses_valid['Epoch'],losses_valid['Loss'], '-r', label='Valid')\n",
    "pl.legend()\n",
    "pl.show()\n",
    "\n",
    "print('Training loss: {:.2f}'.format(losses_train['Loss'].iloc[-1],))\n",
    "print('Validation loss:{:.2f}\\tValidation MAP: {:.2f}'.format(losses_valid['Loss'].iloc[-1],accuracy))\n",
    "\n",
    "print(\"\\n\\t\\t\\t\\t\\tTraining time {}\".format(datetime.datetime.now()-now))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
